## Game API with Kafka, Spark, Hadoop, Hive, Apache Bench and Presto

### Hanan Sukenik

- In this project, I will be supposedly working as a data scientist for a game development company. I am interested in two events tracked by our newest mobile game: `buy a
  sword` & `join guild`

- Through this project, I will:
    * Instrument an API server to log events to Kafka
    * Assemble a data pipeline to catch these events: I will use Spark streaming to filter select event types from Kafka, land them into HDFS/parquet to make them available for analysis using Presto
    * Use Apache Bench to generate test data for your pipeline
    * Produce an analytics report to provide a description of your pipeline and some basic analysis of the events
    
  
- The project includes several files:
    
    * project-3.ipynb - In this Jupyter Notebook, I will walk through the whole process, specify regarding any code used along the way and run the analysis
    * docker-compose.yml - A Docker Compose .yml file used for spinning the pipeline
    * game_api.py- Constucting the API server to log events by using Flask
    * stream_and_hive_guild.py- a Spark script to extract data from Kafka to HDFS and register tables for them with Hive (for the join_guild events)
    * stream_and_hive_sword.py- a Spark script to extract data from Kafka to HDFS and register tables for them with Hive (for the purchase_sword events)
    * gameSim.py- a Bash script for emulating `Purchase Sword` and `Join Guild` events using Apache Bench (Original Code Author: Shane Kramer, Modified by: Hanan Sukenik)
    
    
 - General Notes:
 
     * The eventual analysis is pretty basic as the data is generated by Apache Bench and does not have any true business implications
     
     
Enjoy!
